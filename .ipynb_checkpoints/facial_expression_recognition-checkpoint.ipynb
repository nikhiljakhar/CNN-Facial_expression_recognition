{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "# Install packages correctly\n",
    "!{sys.executable} -m pip install numpy sklearn tensorflow keras\n",
    "# Fix mpl version due to subtle API differences\n",
    "!{sys.executable} -m pip install matplotlib==3.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization,Conv2D,MaxPooling2D\n",
    "from keras.optimizers import RMSprop,SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows,img_cols = 48,48\n",
    "train_dir = './images/train'\n",
    "test_dir = './images/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising train and test data\n",
    "#### Then rescaling them. All the other parameters in ImageDataGenerator for trai data are to create more sample images, like horizontal_flip will make a mirror image. hence increasing the size of out train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageDataGenerator(\n",
    "                rescale=1./255,\n",
    "                rotation_range=30,\n",
    "                shear_range=0.3,\n",
    "                zoom_range=0.3,\n",
    "                width_shift_range=0.3,\n",
    "                height_shift_range=0.3,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True)\n",
    "test_data = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data from the images in our directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28821 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = train_data.flow_from_directory(\n",
    "                                train_dir,\n",
    "                                color_mode='grayscale',\n",
    "                                target_size=(img_rows,img_cols),\n",
    "                                batch_size=8,\n",
    "                                class_mode='categorical',\n",
    "                                shuffle=True)\n",
    "test_gen = test_data.flow_from_directory(\n",
    "                                test_dir,\n",
    "                                color_mode='grayscale',\n",
    "                                target_size=(img_rows,img_cols),\n",
    "                                batch_size=8,\n",
    "                                class_mode='categorical',\n",
    "                                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialising our model to Sequential model provided by keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding 2D convolution layer with 32 as the dimensionality of the output space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah_model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(48,48,1)))\n",
    "mah_model.add(Activation('elu'))\n",
    "mah_model.add(BatchNormalization())\n",
    "mah_model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(48,48,1)))\n",
    "mah_model.add(Activation('elu'))\n",
    "mah_model.add(BatchNormalization())\n",
    "mah_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "mah_model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding 2D convolution layer with 64 as the dimensionality of the output space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah_model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "mah_model.add(Activation('elu'))\n",
    "mah_model.add(BatchNormalization())\n",
    "mah_model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "mah_model.add(Activation('elu'))\n",
    "mah_model.add(BatchNormalization())\n",
    "mah_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "mah_model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding 2D convolution layer with 128 as the dimensionality of the output space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah_model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "mah_model.add(Activation('elu'))\n",
    "mah_model.add(BatchNormalization())\n",
    "mah_model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "mah_model.add(Activation('elu'))\n",
    "mah_model.add(BatchNormalization())\n",
    "mah_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "mah_model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding 2D convolution layer with 256 as the dimensionality of the output space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah_model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "mah_model.add(Activation('elu'))\n",
    "mah_model.add(BatchNormalization())\n",
    "mah_model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "mah_model.add(Activation('elu'))\n",
    "mah_model.add(BatchNormalization())\n",
    "mah_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "mah_model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flattening the output from convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah_model.add(Flatten())\n",
    "mah_model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "mah_model.add(Activation('elu'))\n",
    "mah_model.add(BatchNormalization())\n",
    "mah_model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah_model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "mah_model.add(Activation('elu'))\n",
    "mah_model.add(BatchNormalization())\n",
    "mah_model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax used as activation function as our input is in a range from (0,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah_model.add(Dense(7,kernel_initializer='he_normal'))\n",
    "mah_model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 455       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,167\n",
      "Trainable params: 1,325,991\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mah_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the model with best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'Emotion_detect.h5',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop if the accuracy of our model stops improving after 3 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reduce the learning rate if accuracy is not improving after 3 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_alpha = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_delta=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [checkpoint,earlystop,reduce_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(lr=0.001),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_train_img = 28821\n",
    "no_test_img = 7066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-befdd6a7fce7>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.9900 - accuracy: 0.2125\n",
      "Epoch 00001: val_loss improved from inf to 1.77757, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 666s 185ms/step - loss: 1.9900 - accuracy: 0.2125 - val_loss: 1.7776 - val_accuracy: 0.2574 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.8068 - accuracy: 0.2418\n",
      "Epoch 00002: val_loss did not improve from 1.77757\n",
      "3602/3602 [==============================] - 505s 140ms/step - loss: 1.8068 - accuracy: 0.2418 - val_loss: 1.7847 - val_accuracy: 0.2653 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.7932 - accuracy: 0.2470\n",
      "Epoch 00003: val_loss improved from 1.77757 to 1.76590, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 472s 131ms/step - loss: 1.7932 - accuracy: 0.2470 - val_loss: 1.7659 - val_accuracy: 0.2704 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.7702 - accuracy: 0.2617\n",
      "Epoch 00004: val_loss did not improve from 1.76590\n",
      "3602/3602 [==============================] - 447s 124ms/step - loss: 1.7702 - accuracy: 0.2617 - val_loss: 1.8962 - val_accuracy: 0.2334 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.7583 - accuracy: 0.2714\n",
      "Epoch 00005: val_loss did not improve from 1.76590\n",
      "3602/3602 [==============================] - 443s 123ms/step - loss: 1.7583 - accuracy: 0.2714 - val_loss: 2.0070 - val_accuracy: 0.2296 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.7426 - accuracy: 0.2775\n",
      "Epoch 00006: val_loss improved from 1.76590 to 1.72201, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 443s 123ms/step - loss: 1.7426 - accuracy: 0.2775 - val_loss: 1.7220 - val_accuracy: 0.3232 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.7145 - accuracy: 0.3021\n",
      "Epoch 00007: val_loss improved from 1.72201 to 1.59125, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 444s 123ms/step - loss: 1.7145 - accuracy: 0.3021 - val_loss: 1.5913 - val_accuracy: 0.3863 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.6738 - accuracy: 0.3290\n",
      "Epoch 00008: val_loss improved from 1.59125 to 1.54334, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 448s 124ms/step - loss: 1.6738 - accuracy: 0.3290 - val_loss: 1.5433 - val_accuracy: 0.4002 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.6468 - accuracy: 0.3494\n",
      "Epoch 00009: val_loss improved from 1.54334 to 1.47722, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 469s 130ms/step - loss: 1.6468 - accuracy: 0.3494 - val_loss: 1.4772 - val_accuracy: 0.4434 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.6204 - accuracy: 0.3656\n",
      "Epoch 00010: val_loss improved from 1.47722 to 1.45377, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 510s 142ms/step - loss: 1.6204 - accuracy: 0.3656 - val_loss: 1.4538 - val_accuracy: 0.4517 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.6032 - accuracy: 0.3692\n",
      "Epoch 00011: val_loss improved from 1.45377 to 1.41913, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 499s 139ms/step - loss: 1.6032 - accuracy: 0.3692 - val_loss: 1.4191 - val_accuracy: 0.4563 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.5859 - accuracy: 0.3793\n",
      "Epoch 00012: val_loss improved from 1.41913 to 1.38444, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 562s 156ms/step - loss: 1.5859 - accuracy: 0.3793 - val_loss: 1.3844 - val_accuracy: 0.4725 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.5674 - accuracy: 0.3861\n",
      "Epoch 00013: val_loss improved from 1.38444 to 1.37727, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 487s 135ms/step - loss: 1.5674 - accuracy: 0.3861 - val_loss: 1.3773 - val_accuracy: 0.4635 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.5560 - accuracy: 0.3898\n",
      "Epoch 00014: val_loss did not improve from 1.37727\n",
      "3602/3602 [==============================] - 483s 134ms/step - loss: 1.5560 - accuracy: 0.3898 - val_loss: 1.4127 - val_accuracy: 0.4689 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.5492 - accuracy: 0.3961\n",
      "Epoch 00015: val_loss did not improve from 1.37727\n",
      "3602/3602 [==============================] - 463s 128ms/step - loss: 1.5492 - accuracy: 0.3961 - val_loss: 1.3916 - val_accuracy: 0.4744 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.5393 - accuracy: 0.4003\n",
      "Epoch 00016: val_loss improved from 1.37727 to 1.35361, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 483s 134ms/step - loss: 1.5393 - accuracy: 0.4003 - val_loss: 1.3536 - val_accuracy: 0.4887 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.5283 - accuracy: 0.4057\n",
      "Epoch 00017: val_loss did not improve from 1.35361\n",
      "3602/3602 [==============================] - 506s 141ms/step - loss: 1.5283 - accuracy: 0.4057 - val_loss: 1.4053 - val_accuracy: 0.4723 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.5167 - accuracy: 0.4107\n",
      "Epoch 00018: val_loss improved from 1.35361 to 1.31444, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 469s 130ms/step - loss: 1.5167 - accuracy: 0.4107 - val_loss: 1.3144 - val_accuracy: 0.5004 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.5033 - accuracy: 0.4192\n",
      "Epoch 00019: val_loss improved from 1.31444 to 1.28937, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 4029s 1s/step - loss: 1.5033 - accuracy: 0.4192 - val_loss: 1.2894 - val_accuracy: 0.5095 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.4896 - accuracy: 0.4235\n",
      "Epoch 00020: val_loss improved from 1.28937 to 1.25479, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 463s 129ms/step - loss: 1.4896 - accuracy: 0.4235 - val_loss: 1.2548 - val_accuracy: 0.5202 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.4785 - accuracy: 0.4307\n",
      "Epoch 00021: val_loss did not improve from 1.25479\n",
      "3602/3602 [==============================] - 462s 128ms/step - loss: 1.4785 - accuracy: 0.4307 - val_loss: 1.2773 - val_accuracy: 0.5177 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.4696 - accuracy: 0.4344\n",
      "Epoch 00022: val_loss did not improve from 1.25479\n",
      "3602/3602 [==============================] - 479s 133ms/step - loss: 1.4696 - accuracy: 0.4344 - val_loss: 1.2549 - val_accuracy: 0.5221 - lr: 0.0010\n",
      "Epoch 23/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.4652 - accuracy: 0.4395\n",
      "Epoch 00023: val_loss improved from 1.25479 to 1.23754, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 528s 147ms/step - loss: 1.4652 - accuracy: 0.4395 - val_loss: 1.2375 - val_accuracy: 0.5300 - lr: 0.0010\n",
      "Epoch 24/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.4532 - accuracy: 0.4465\n",
      "Epoch 00024: val_loss improved from 1.23754 to 1.23313, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 500s 139ms/step - loss: 1.4532 - accuracy: 0.4465 - val_loss: 1.2331 - val_accuracy: 0.5269 - lr: 0.0010\n",
      "Epoch 25/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.4513 - accuracy: 0.4474\n",
      "Epoch 00025: val_loss improved from 1.23313 to 1.20802, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 533s 148ms/step - loss: 1.4513 - accuracy: 0.4474 - val_loss: 1.2080 - val_accuracy: 0.5330 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.4384 - accuracy: 0.4507\n",
      "Epoch 00026: val_loss improved from 1.20802 to 1.20131, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 497s 138ms/step - loss: 1.4384 - accuracy: 0.4507 - val_loss: 1.2013 - val_accuracy: 0.5303 - lr: 0.0010\n",
      "Epoch 27/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.4356 - accuracy: 0.4522\n",
      "Epoch 00027: val_loss improved from 1.20131 to 1.18878, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 463s 129ms/step - loss: 1.4356 - accuracy: 0.4522 - val_loss: 1.1888 - val_accuracy: 0.5487 - lr: 0.0010\n",
      "Epoch 28/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.4277 - accuracy: 0.4549\n",
      "Epoch 00028: val_loss did not improve from 1.18878\n",
      "3602/3602 [==============================] - 465s 129ms/step - loss: 1.4277 - accuracy: 0.4549 - val_loss: 1.2066 - val_accuracy: 0.5320 - lr: 0.0010\n",
      "Epoch 29/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.4230 - accuracy: 0.4591\n",
      "Epoch 00029: val_loss improved from 1.18878 to 1.18358, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 460s 128ms/step - loss: 1.4230 - accuracy: 0.4591 - val_loss: 1.1836 - val_accuracy: 0.5524 - lr: 0.0010\n",
      "Epoch 30/30\n",
      "3602/3602 [==============================] - ETA: 0s - loss: 1.4091 - accuracy: 0.4673\n",
      "Epoch 00030: val_loss improved from 1.18358 to 1.17695, saving model to Emotion_detect.h5\n",
      "3602/3602 [==============================] - 479s 133ms/step - loss: 1.4091 - accuracy: 0.4673 - val_loss: 1.1770 - val_accuracy: 0.5488 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "fit_model = mah_model.fit_generator(\n",
    "                train_gen,\n",
    "                steps_per_epoch=no_train_img//8,\n",
    "                epochs=30,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=test_gen,\n",
    "                validation_steps=no_test_img//8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
